import argparse
import os 
os.environ['TOKENIZERS_PARALLELISM'] = 'false'


import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from torch.nn import functional as nnf

from model.model_with_prompt import PromptModel, PromptModelWithConnection
from datasets import ImageCaptionDataset, prepare_data
from utils import CosineSchedule, generate, calculate_metrics, save_info, \
                save_config_and_metric, get_optimizer, get_dataloader, process_args
from transformers import get_linear_schedule_with_warmup

def train(args, train_dataset, valid_dataset, model):
    print(args)
    batch_size = args.bs
    device = args.device
    epochs = args.epochs
    model = model.to(device)
    
    optimizer = get_optimizer(args,model)
    train_dataloader, valid_dataloader = get_dataloader(args, train_dataset, valid_dataset)
    if args.scheduler_type == 'cosine':
        scheduler = CosineSchedule(optimizer, K=args.scheduler_k)
    elif args.scheduler_type == 'linear':
        num_steps = len(train_dataloader)*epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_steps//10, num_steps)
    else:
        raise ValueError(f'Not support {args.scheduler_type}')
    writer = SummaryWriter(args.out_dir)

    # TRAINING + VALIDATION LOOP
    best_epoch = -1
    highest_avg = -1
    best_metrics = None
    for epoch in range(epochs):
        for param_group in optimizer.param_groups:
            lr = param_group["lr"]
            print(f'>>> Training epoch {epoch} - LR: {lr}')
        progress = tqdm(total=len(train_dataloader))
        total_train_loss = 0

        # Training loop
        model.train()
        for idx, (img_path, img_tensor, hard_text_prompt, caption) in enumerate(train_dataloader):
            """
            - img_path: tuple, len = batch_size
            - img_tensors: tensor, shape (bs, c, w, h)
            - caption: tuple, len = batch_size
            """
            
            model.zero_grad()
            # prepare inputs
            img_tensor = img_tensor.to(device, dtype=torch.float32)

            # get query for prompt and calculate a similarity loss
            q = model.get_query(img_tensor)
            n_K = nn.functional.normalize(model.key, dim=1)
            q = nn.functional.normalize(q, dim=1).detach()
            cos_sim = torch.einsum('bj,kj->bk', q, n_K)
            loss_1 = 1.0 - sum(cos_sim)/batch_size
            
            # forward
            outputs = model(img_tensor, caption)

            # get result
            output_logits = outputs['last_layer_logits'] # bs, seq_len, vocab_size
            token_ids = outputs['input_ids']             # generated by tokenizer, used as a target in the loss
            
            # causal model => shift input and logits
            if args.decoder_type == 'gpt2':
                hard_prompt_len = model.tokenizer(hard_text_prompt[0], return_tensors="pt").input_ids.shape[1]
            elif args.decoder_type == 'd_plip':
                hard_prompt_len = model.tokenizer(hard_text_prompt[0], return_tensors="pt").input_ids.shape[1] - 1 
            shift_logits = output_logits[..., hard_prompt_len-1:-1, :].contiguous()   # skip the last token and hard-prompt tokens
            shift_labels = token_ids[..., hard_prompt_len:].contiguous()              # skip the first token_id (bos) and hard-prompt

            # calculate loss
            loss_2 = nnf.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            loss = loss_1 + loss_2
            total_train_loss += loss_1.item() + loss_2.item()

            # backprop
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            progress.set_postfix({"loss": loss.item()})
            progress.update()
            if args.scheduler_type == 'linear':
                scheduler.step()
        if args.scheduler_type == 'cosine':
            scheduler.step()
        progress.close()

        torch.save({
            'epoch': epoch,
            'key': model.key,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'schedulerr_state_dict': scheduler.state_dict(),
            'loss': loss,
            }, 
            os.path.join(args.out_dir, f"{args.prefix_outdir}-{epoch}.pt"),
        )

        ground_truth_list = []
        prediction_list = []

        # Validation loop
        if epoch % args.valid_every == 0:
            model.eval()
            print(f">>> Evaluating epoch {epoch}")
            progress = tqdm(total=len(valid_dataloader))
            with torch.no_grad():
                for _, (img_path, img_tensor, hard_text_prompt, caption) in enumerate(valid_dataloader):
                    img_tensor = img_tensor.to(device, dtype=torch.float32)  # bs x 3 x 512 x 512                    
                    gen_cap = generate(model, img_tensor, hard_text_prompt, args)
                    ground_truth_list += caption
                    prediction_list += gen_cap
                    progress.update()
            progress.close()
        
            assert len(ground_truth_list) == len(prediction_list)
            # Log info to writer
            log_info = {}
            log_info['val_metrics'] = calculate_metrics(args.dataset, ground_truth_list, prediction_list)
            print(log_info['val_metrics'])
            log_info['lr'] = lr
            log_info['train_loss'] = total_train_loss/len(train_dataset)
            log_info['ground_truth_list'] = ground_truth_list
            log_info['prediction_list'] = prediction_list
            save_info(args, log_info, writer, epoch)
            if log_info['val_metrics']['valid_avg'] > highest_avg:
                highest_avg = log_info['val_metrics']['valid_avg']
                best_epoch = epoch
                best_metrics = log_info['val_metrics']
    
    print(args)
    print('FINISHED !!!!')
    print(f'Best epoch: {best_epoch}')
    print(best_metrics)
    save_config_and_metric(args, best_metrics, best_epoch)

    return model


def main():
    parser = argparse.ArgumentParser()

    # Dataset
    parser.add_argument('--dataset', choices=['colon-1', 'prostate-1', 'gastric'],default='prostate-1')

    # Training configuaration
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--bs', type=int, default=256)
    parser.add_argument('--device', type=int, default=7)
    parser.add_argument('--optimizer_type', type=str, default="AdamW")
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--betas', type=tuple, default=(0.9, 0.999))
    parser.add_argument('--valid_every', type=int, default=1)
    parser.add_argument('--num_workers', type=int, default=10)
    parser.add_argument('--scheduler_type', type=str, default="linear")
    parser.add_argument('--scheduler_k', type=int, default=50)
    parser.add_argument('--generate_length', type=int, default=6)

    # Adapt methods
    parser.add_argument('--prompt_type', type=str, choices=['basic', 'distinct', 'connection', 'lora'],default='lora')

    # Lora
    parser.add_argument('--lora_r', type=int, default=6)
    parser.add_argument('--lora_alpha', type=int, default=12)
    parser.add_argument('--lora_drop_out', type=float, default=0.1)
    parser.add_argument('--encoder_lora_skip_layers', type=list, default=[6,7,8,9,10,11])
    parser.add_argument('--decoder_lora_skip_layers', type=list, default=[6,7,8,9,10,11])

    # Prompt
    parser.add_argument('--encoder_prompt_len', type=int, default=10)
    parser.add_argument('--encoder_skip_layers', type=list, default=[6,7,8,9,10,11])
    parser.add_argument('--decoder_prompt_len', type=int, default=10)
    parser.add_argument('--decoder_skip_layers', type=list, default=[6,7,8,9,10,11]) # skip for prompt
    parser.add_argument('--decoder_skip_layers_for_visual', type=list, default=[6,7,8,9,10,11])   # skip for visual feature

    # Encoder
    parser.add_argument('--encoder_type', type=str, default='ctranspath')
    parser.add_argument('--encoder_ckpt_path', type=str, default='/home/compu/anhnguyen/prompt_works/model/ctranspath.pth')
    parser.add_argument('--encoder_resize', default=224)
    parser.add_argument('--encoder_mean', default=(0.485, 0.456, 0.406))
    parser.add_argument('--encoder_std', default=(0.229, 0.224, 0.225))

    # Projector
    parser.add_argument('--layers_dim', type=list, default=[768, 1024, 4096, 2048, 512])
    parser.add_argument('--proj_activation', type=str, default='gelu')
    
    # Decoder
    parser.add_argument('--decoder_type', type=str, default='d_plip')
    parser.add_argument('--decoder_ckpt_path', type=str, default='/home/compu/anhnguyen/prompt_works/model/plip_ckpt')
    parser.add_argument('--tokenizer_type', type=str, default="vinid/plip")

    # Saving configuration
    parser.add_argument('--out_dir', default='/data4/anhnguyen/experiments/prompt_work/')
    parser.add_argument('--prefix_outdir', type=str, default="")

    args = parser.parse_args()
    process_args(args)
    
    data = prepare_data(args.dataset)
    if isinstance(data,tuple):
        train_set, valid_set = data[0], data[1]
    else:
        raise ValueError('Not contains a splitted training data')
    
    if args.prompt_type == 'connection':
        model = PromptModelWithConnection(args)
    else:
        model = PromptModel(args)
    train_dataset = ImageCaptionDataset(train_set, args)
    valid_dataset = ImageCaptionDataset(valid_set, args, train=False)

    train(args, train_dataset, valid_dataset, model)

if __name__ == '__main__':
    main()