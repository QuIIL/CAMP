import argparse
import os 
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from torch.nn import functional as nnf
from torch.utils.data import DataLoader
from model.model_with_prompt import PromptModel
from datasets import ImageCaptionDataset, prepare_data
from utils import CosineSchedule, generate, calculate_metrics, save_config, save_info

def train(args, train_dataset, valid_dataset, model):
    print(args)
    batch_size = args.bs
    device = args.device
    epochs = args.epochs
    model = model.to(device)
    
    model.train()
    if args.optimizer_type == 'Adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=args.betas)
    else:
        raise NotImplementedError

    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=args.num_workers)
    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=args.num_workers)

    scheduler = CosineSchedule(optimizer, K=args.scheduler_k)
    writer = SummaryWriter(args.out_dir)
    cos = nn.CosineSimilarity(dim=1)

    # TRAINING + VALIDATION LOOP 
    for epoch in range(epochs):
        for param_group in optimizer.param_groups:
            lr = param_group["lr"]
            print(f'>>> Training epoch {epoch} - LR: {lr}')
        progress = tqdm(total=len(train_dataloader))
        total_train_loss = 0

        # Training loop
        for idx, (img_path, img_tensor, hard_text_prompt, caption) in enumerate(train_dataloader):
            """
            - img_path: tuple, len = batch_size
            - img_tensors: tensor, shape (bs, c, w, h)
            - caption: tuple, len = batch_size
            """
            model.zero_grad()
            # prepare inputs
            img_tensor = img_tensor.to(device, dtype=torch.float32)

            # get query for prompt and calculate a similarity loss
            q = model.get_query(img_tensor)
            n_K = nn.functional.normalize(model.key, dim=1)
            q = nn.functional.normalize(q, dim=1).detach()
            cos_sim = torch.einsum('bj,kj->bk', q, n_K)
            loss_1 = 1.0 - sum(cos_sim)/batch_size
            
            # forward
            outputs = model(img_tensor, caption)

            # get result
            output_logits = outputs['last_layer_logits'] # bs, seq_len, vocab_size
            token_ids = outputs['input_ids']             # generated by tokenizer, used as a target in the loss

            # causal model => shift input and logits
            shift_logits = output_logits[..., :-1, :].contiguous()   # skip the last token_embed
            shift_labels = token_ids[..., 1:].contiguous()           # skip the first token_id (bos)

            # calculate loss
            loss_2 = nnf.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            loss = loss_1 + loss_2
            total_train_loss += loss_1.item() + loss_2.item()

            # backprop
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            progress.set_postfix({"loss": loss.item()})
            progress.update()
        scheduler.step()
        progress.close()

        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'schedulerr_state_dict': scheduler.state_dict(),
            'loss': loss,
            }, 
            os.path.join(args.out_dir, f"{args.prefix_outdir}-{epoch}.pt"),
        )

        ground_truth_list = []
        prediction_list = []

        # Validation loop
        if epoch % args.valid_every == 0:
            print(f">>> Evaluating epoch {epoch}")
            progress = tqdm(total=len(valid_dataloader))
            with torch.no_grad():
                for _, (img_path, img_tensor, hard_text_prompt, caption) in enumerate(valid_dataloader):
                    img_tensor = img_tensor.to(device, dtype=torch.float32)  # bs x 3 x 512 x 512                    
                    gen_cap = generate(model, img_tensor, hard_text_prompt, args)
                    ground_truth_list += caption
                    prediction_list += gen_cap
                    progress.update()
            progress.close()
        
        assert len(ground_truth_list) == len(prediction_list)
        # Log info to writer
        print(ground_truth_list[:5])
        print(prediction_list[:5])
        log_info = {}
        log_info['val_metrics'] = calculate_metrics(args.dataset, ground_truth_list, prediction_list)
        log_info['lr'] = lr
        log_info['train_loss'] = total_train_loss/len(train_dataset)
        log_info['ground_truth_list'] = ground_truth_list
        log_info['prediction_list'] = prediction_list
        save_info(args, log_info, writer, epoch)

    return model


def main():
    now = datetime.now()

    parser = argparse.ArgumentParser()

    # Dataset
    parser.add_argument('--dataset', default='colon-1')

    # Training configuaration
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--bs', type=int, default=512)
    parser.add_argument('--device', type=int, default=2)
    parser.add_argument('--optimizer_type', type=str, default="Adam")
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--betas', type=tuple, default=(0.9, 0.999))
    parser.add_argument('--valid_every', type=int, default=1)
    parser.add_argument('--num_workers', type=int, default=40)
    parser.add_argument('--scheduler_k', type=int, default=50)
    parser.add_argument('--generate_length', type=int, default=6)

    # Encoder
    parser.add_argument('--encoder_type', type=str, default='ctranspath')
    parser.add_argument('--encoder_prompt_len', type=int, default=1)
    parser.add_argument('--encoder_ckpt_path', type=str, default='/home/compu/anhnguyen/TransPath/ctranspath.pth')
    parser.add_argument('--encoder_skip_layers', type=list, default=[])
    parser.add_argument('--encoder_resize', default=224)
    parser.add_argument('--encoder_mean', default=(0.485, 0.456, 0.406))
    parser.add_argument('--encoder_std', default=(0.229, 0.224, 0.225))

    # Projector
    parser.add_argument('--layers_dim', type=list, default=[768, 1024, 512])
    parser.add_argument('--proj_activation', type=str, default='gelu')
    
    # Decoder
    parser.add_argument('--decoder_type', type=str, default='plip')
    parser.add_argument('--decoder_prompt_len', type=int, default=1)
    parser.add_argument('--decoder_ckpt_path', type=str, default='/home/compu/anhnguyen/prompt_works/plip_ckpt')
    parser.add_argument('--decoder_skip_layers', type=list, default=[])
    parser.add_argument('--tokenizer_type', type=str, default="vinid/plip")

    # Saving configuration
    parser.add_argument('--out_dir', default='/data4/anhnguyen/experiments/prompt_work/')
    parser.add_argument('--prefix_outdir', type=str, default="")

    args = parser.parse_args()

    log_skip_layers_encoder = ''.join(args.enconder_skip_layers) if args.enconder_skip_layers != [] else 'full'
    log_skip_layers_decoder = ''.join(args.decoder_skip_layers) if args.decoder_skip_layers != [] else 'full'
    
    args.prefix_outdir = '-'.join((args.dataset, 
                                    args.encoder_type,
                                    str(args.enconder_prompt_len),
                                    log_skip_layers_encoder,
                                    args.decoder_type,
                                    str(args.decoder_prompt_len),
                                    log_skip_layers_decoder,
                                    str(now)[-3:]
                                    ))

    args.out_dir = args.out_dir + args.prefix_outdir 

    if not os.path.exists(args.out_dir):
        os.makedirs(args.out_dir)
    
    save_config(args)

    args.device = torch.device(f'cuda:{args.device}')
    
    data = prepare_data(args.dataset)
    if isinstance(data,tuple):
        train_set, valid_set, test_set = data[0], data[1], data[2]
    else:
        test_set = data
    
    model = PromptModel(args)
    train_dataset = ImageCaptionDataset(train_set, args)
    valid_dataset = ImageCaptionDataset(valid_set, args)

    train(args, train_dataset, valid_dataset, model)

if __name__ == '__main__':
    main()