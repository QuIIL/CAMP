import argparse
import os 
os.environ['TOKENIZERS_PARALLELISM'] = 'false'


import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from torch.nn import functional as nnf

from model.model_with_prompt import PromptModel
from model.single_encoder import SingleEncoder
from datasets import ImageDataset, prepare_data
from utils import CosineSchedule, generate, calculate_metrics, save_info, \
                save_config_and_metric, get_optimizer, get_dataloader, \
                process_args, loss_key, loss_caption, get_num_class
from transformers import get_linear_schedule_with_warmup

def train(args, train_dataset, valid_dataset, model):
    print(args)
    batch_size = args.bs
    device = args.device
    epochs = args.epochs
    model = model.to(device)
    
    optimizer = get_optimizer(args,model)
    train_dataloader, valid_dataloader = get_dataloader(args, train_dataset, valid_dataset)
    if args.scheduler_type == 'cosine':
        scheduler = CosineSchedule(optimizer, K=args.scheduler_k)
    elif args.scheduler_type == 'linear':
        num_steps = len(train_dataloader)*epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, int(num_steps*args.warmup_ratio), num_steps)
    elif args.scheduler_type == 'cosine_restart':
        import torch.optim as optim
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=args.epochs // 2, T_mult=1,
                                                                   eta_min=args.lr * 0.1, last_epoch=-1)
    else:
        raise ValueError(f'Not support {args.scheduler_type}')
    writer = SummaryWriter(args.out_dir)

    # TRAINING + VALIDATION LOOP
    best_epoch = -1
    highest_avg = -1
    best_metrics = None
    for epoch in range(epochs):
        for param_group in optimizer.param_groups:
            lr = param_group["lr"]
            print(f'>>> Training epoch {epoch} - LR: {lr}')
        progress = tqdm(total=len(train_dataloader))
        total_train_loss = 0

        # Training loop
        model.train()
        for idx, (img_path, img_tensor, hard_text_prompt, label) in enumerate(train_dataloader):
            """
            - img_path: tuple, len = batch_size
            - img_tensors: tensor, shape (bs, c, w, h)
            - caption: tuple, len = batch_size
            """   
            model.zero_grad()
            img_tensor = img_tensor.to(device, dtype=torch.float32)
            
            # get similarity loss
            if args.type not in ['full_ft', 'single_encoder']:
                loss_1 = loss_key(model,img_tensor, hard_text_prompt, batch_size)
            
            # forward
            if args.type != 'single_encoder':
                outputs = model(img_tensor, label)
                output_logits = outputs['last_layer_logits'] # bs, seq_len, vocab_size
                token_ids = outputs['input_ids']             # generated by tokenizer, used as a target in the loss
                loss_2 = loss_caption(args, model, hard_text_prompt, output_logits, token_ids)
            else:
                label = label.to(device)
                outputs = model(img_tensor)
                loss_2 = nnf.cross_entropy(outputs, label)
            
            if 'loss_1' in locals():  # if loss exists
                loss = loss_1 + loss_2
                total_train_loss += loss_1.item() + loss_2.item()
            else:
                loss = loss_2
                total_train_loss += loss_2.item()

            # backprop
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            progress.set_postfix({"loss": loss.item()})
            progress.update()
            if args.scheduler_type == 'linear':
                scheduler.step()
        if args.scheduler_type in ['cosine','cosine_restart']:
            scheduler.step()
        progress.close()

        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'schedulerr_state_dict': scheduler.state_dict(),
            'loss': loss,
            }, 
            os.path.join(args.out_dir, f"{args.prefix_outdir}-{epoch}.pt"),
        )

        ground_truth_list = []
        prediction_list = []

        # Validation loop
        if epoch % args.valid_every == 0:
            model.eval()
            print(f">>> Evaluating epoch {epoch}")
            progress = tqdm(total=len(valid_dataloader))
            with torch.no_grad():
                for _, (img_path, img_tensor, hard_text_prompt, label) in enumerate(valid_dataloader):
                    img_tensor = img_tensor.to(device, dtype=torch.float32)  # bs x 3 x 512 x 512
                    if args.type != 'single_encoder':                    
                        gen_cap = generate(model, img_tensor, hard_text_prompt, args)
                        ground_truth_list += label
                        prediction_list += gen_cap
                    else:
                        outputs = model(img_tensor)
                        ground_truth_list += label.tolist()
                        prediction_list += torch.argmax(outputs, dim=1).tolist()
                    progress.update()
            progress.close()
        
            assert len(ground_truth_list) == len(prediction_list)
            # Log info to writer
            log_info = {}
            log_info['val_metrics'] = calculate_metrics(args.dataset, ground_truth_list, prediction_list)
            print(log_info['val_metrics'])
            log_info['lr'] = lr
            log_info['train_loss'] = total_train_loss/len(train_dataset)
            log_info['ground_truth_list'] = ground_truth_list
            log_info['prediction_list'] = prediction_list
            save_info(args, log_info, writer, epoch)
            if log_info['val_metrics']['valid_avg'] > highest_avg:
                highest_avg = log_info['val_metrics']['valid_avg']
                best_epoch = epoch
                best_metrics = log_info['val_metrics']
    
    print(args)
    print('FINISHED !!!!')
    print(f'Best epoch: {best_epoch}')
    print(best_metrics)
    save_config_and_metric(args, best_metrics, best_epoch)

    return model


def main():
    parser = argparse.ArgumentParser()

    # Dataset
    parser.add_argument('--dataset', choices=['colon-1', 'prostate-1', 'gastric', 'k19',
                                              'liver', 'kidney','breakhis','bladder','bach',
                                              'pcam','panda', 'medfm', 'unitopath', 'luad'],default='colon-1')
    parser.add_argument('--breakhis_fold', type=int, default=1)

    # Training configuaration
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--bs', type=int, default=256)
    parser.add_argument('--device', type=int, default=2)
    parser.add_argument('--optimizer_type', type=str, default="Adam")
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--momentum', type=float, default=0)
    parser.add_argument('--betas', type=tuple, default=(0.9, 0.999))
    parser.add_argument('--valid_every', type=int, default=1)
    parser.add_argument('--num_workers', type=int, default=10)
    parser.add_argument('--scheduler_type', type=str, default="cosine")
    parser.add_argument('--warmup_ratio', type=float, default=0.1)
    parser.add_argument('--scheduler_k', type=int, default=50)
    parser.add_argument('--generate_length', type=int, default=12)

    # Adapt methods
    parser.add_argument('--type', type=str, choices=['basic', 'distinct',
                                                     'connection', 'lora',
                                                     'full_ft', 'single_encoder'],default='lora')

    # Lora
    parser.add_argument('--lora_r', type=int, default=6)
    parser.add_argument('--lora_alpha', type=int, default=12)
    parser.add_argument('--lora_drop_out', type=float, default=0.1)
    parser.add_argument('--encoder_lora_skip_layers', type=list, default=[6,7,8,9,10,11])
    parser.add_argument('--decoder_lora_skip_layers', type=list, default=[6,7,8,9,10,11])

    # Prompt
    parser.add_argument('--encoder_prompt_len', type=int, default=10)
    parser.add_argument('--encoder_skip_layers', type=list, default=[6,7,8,9,10,11])
    parser.add_argument('--decoder_prompt_len', type=int, default=10)
    parser.add_argument('--decoder_skip_layers', type=list, default=[6,7,8,9,10,11]) # skip for prompt
    parser.add_argument('--decoder_skip_layers_for_visual', type=list, default=[6,7,8,9,10,11])   # skip for visual feature

    # Encoder
    # parser.add_argument('--encoder_type', type=str, default='swin_tiny')
    # parser.add_argument('--encoder_ckpt_path', type=str, default='/home/compu/anhnguyen/prompt_works/model/swin_tiny_patch4_window7_224.pth')

    parser.add_argument('--encoder_type', type=str, default='resnet50')
    parser.add_argument('--encoder_ckpt_path', type=str, default='/home/compu/anhnguyen/prompt_works/model/ctranspath.pth')

    parser.add_argument('--encoder_resize', type=int, default=224)
    parser.add_argument('--encoder_mean', default=(0.485, 0.456, 0.406))
    parser.add_argument('--encoder_std', default=(0.229, 0.224, 0.225))

    # Projector
    parser.add_argument('--layers_dim', type=list, default=[768, 1024, 4096, 2048, 768])
    parser.add_argument('--proj_activation', type=str, default='gelu')
    
    # Decoder
    parser.add_argument('--decoder_type', type=str, default='gpt2')
    parser.add_argument('--decoder_ckpt_path', type=str, default='/home/compu/anhnguyen/prompt_works/model/gpt2_ckpt')
    parser.add_argument('--tokenizer_type', type=str, default="gpt2")

    # parser.add_argument('--decoder_type', type=str, default='d_plip')
    # parser.add_argument('--decoder_ckpt_path', type=str, default='/home/compu/anhnguyen/prompt_works/model/plip_ckpt')
    # parser.add_argument('--tokenizer_type', type=str, default="vinid/plip")

    # Saving configuration
    parser.add_argument('--out_dir', default='/data4/anhnguyen/experiments/prompt_work/')
    parser.add_argument('--prefix_outdir', type=str, default="")

    args = parser.parse_args()
    process_args(args)
    
    data = prepare_data(args)
    if isinstance(data,tuple):
        train_set, valid_set = data[0], data[1]
    else:
        raise ValueError('Not contains a splitted training data')

    if args.encoder_type == 'uni':
        transform_train = transforms.Compose(
        [   
            transforms.ToTensor(),
            transforms.Resize((224,224)),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ]
        )
        transform_test = transform_train
        args.layers_dim = [1024, 2048, 4096, 2048, 768]
        print(args.layers_dim)
    elif args.encoder_type == 'quiltnet':
        _ , transform_train, transform_test = open_clip.create_model_and_transforms('hf-hub:wisdomik/QuiltNet-B-16-PMB')
    elif args.encoder_type == 'phikon':
        from transformers import AutoImageProcessor
        transform_train = AutoImageProcessor.from_pretrained("owkin/phikon")
        transform_test = transform_train
        args.layers_dim = [768, 1024, 4096, 2048, 768]
    elif args.encoder_type == 'ctranspath':
        transform_train = transforms.Compose([
            transforms.RandomResizedCrop((224, 224), scale=(0.05, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),
        ])
        transform_test = transforms.Compose([
            transforms.Resize(int((256 / 224) * 224)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),
        ])
    
    if args.type == 'single_encoder':
        model = SingleEncoder(args)
    else:
        model = PromptModel(args)
    
    if args.type != 'single_encoder':
        model = PromptModel(args)
    elif args.type == 'single_encoder':
        model = SingleEncoder(args, get_num_class(args.dataset))
    
    train_dataset = ImageDataset(train_set, args, transform=transform_train)
    valid_dataset = ImageDataset(valid_set, args, transform=transform_test)

    train(args, train_dataset, valid_dataset, model)

if __name__ == '__main__':
    main()
